diff --git Makefile Makefile
new file mode 100644
index 0000000..49c52bd
--- /dev/null
+++ Makefile
@@ -0,0 +1,32 @@
+.PHONY: release release-minor release-path release-major upload test test-all lint clean
+
+VERSION?=minor
+
+release:
+	bumpversion $(VERSION)
+
+release-minor: release
+
+release-patch:
+	make release VERSION=patch
+
+release-major:
+	make release VERSION=major
+
+upload: clean
+	python setup.py sdist bdist_wheel upload
+
+test:
+	py.test tests/
+
+test-all:
+	tox
+
+lint:
+	flake8 hubstorage
+
+clean:
+	rm -rf build/ dist/ *.egg-info htmlcov/
+	find . -name '*.pyc' -exec rm -f {} +
+	find . -name '*.pyo' -exec rm -f {} +
+	find . -name '__pycache__' -exec rm -fr {} +
diff --git README.rst README.rst
index f6ea6fb..5b53b08 100644
--- README.rst
+++ README.rst
@@ -25,7 +25,7 @@ Testing
 -------
 
 Running the tests require the hubstorage backend to be running,
-and the python `responses` library (see `tox.ini`).
+and the python `responses` library (see `requirements-test.txt`).
 
 Usage
 ---------
@@ -51,7 +51,7 @@ To get project settings or jobs summary:
  u'pending': 0,
  u'project': 1111111,
  u'running': 0}
- 
+
 Spider
 **********
 
@@ -94,7 +94,8 @@ To push job from project level with the highest priority:
 4
 
 Pushing a job with spider arguments:
- >>> project.push_job(spidername='foo', spider_args={'arg1': 'foo', 'arg2': 'bar'})
+
+>>> project.push_job(spidername='foo', spider_args={'arg1': 'foo', 'arg2': 'bar'})
 
 Running job can be **cancelled** by calling ``request_cancel()``:
 
@@ -132,7 +133,7 @@ u'memusage/startup': 62439424,
 
 Anything can be stored in metadata, here is example how to add tags:
 
->>> job.metadata.update_metadata({'tags': 'obsolete'})
+>>> job.update_metadata({'tags': 'obsolete'})
 
 Jobs
 *************
@@ -147,11 +148,11 @@ Jobq metadata fieldset is less detailed, than ``job.metadata``, but contains few
 Additional fields can be requested using the ``jobmeta`` parameter.
 If it used, then it's up to the user to list all the required fields, so only few default fields would be added except requested ones.
 
->>> metadata = project.jobq.list().next()
+>>> metadata = next(project.jobq.list())
 >>> metadata.get('spider', 'missing')
 u'foo'
 >>> jobs_metadata = project.jobq.list(jobmeta=['scheduled_by', ])
->>> metadata = jobs_metadata.next()
+>>> metadata = next(jobs_metadata)
 >>> metadata.get('scheduled_by', 'missing')
 u'John'
 >>> metadata.get('spider', 'missing')
@@ -166,6 +167,8 @@ To get jobs filtered by tags:
 
 >>> jobs_metadata = project.jobq.list(has_tag=['new', 'verified'], lacks_tag='obsolete')
 
+List of tags has ``OR`` power, so in the case above jobs with 'new' or 'verified' tag are expected.
+
 To get certain number of last finished jobs per some spider:
 
 >>> jobs_metadata = project.jobq.list(spider='foo', state='finished' count=3)
@@ -185,7 +188,7 @@ To iterate through items:
 >>> items = job.items.iter_values()
 >>> for item in items:
    # do something, item is just a dict
-   
+
 Logs
 ********
 
@@ -194,7 +197,7 @@ To iterate through 10 first logs for example:
 >>> logs = job.logs.iter_values(count=10)
 >>> for log in logs:
    # do something, log is a dict with log level, message and time keys
-   
+
 Collections
 **************
 
@@ -213,7 +216,44 @@ Let's store hash and timestamp pair for foo spider. Usual workflow with `Collect
 >>> foo_store.count()
 0
 
+Frontier
+**************
+
+Typical workflow with `Frontier`_:
+
+>>> frontier = project.frontier
+
+Add a request to the frontier:
+
+>>> frontier.add('test', 'example.com', [{'fp': '/some/path.html'}])
+>>> frontier.flush()
+>>> frontier.newcount
+1
+
+Add requests with additional parameters:
+
+>>> frontier.add('test', 'example.com', [{'fp': '/'}, {'fp': 'page1.html', 'p': 1, 'qdata': {'depth': 1}}])
+>>> frontier.flush()
+>>> frontier.newcount
+2
+
+To delete the slot ``example.com`` from the frontier:
+
+>>> frontier.delete_slot('test', 'example.com')
+
+To retrieve requests for a given slot:
+
+>>> reqs = frontier.read('test', 'example.com')
+
+To delete a batch of requests:
+
+>>> frontier.delete('test', 'example.com', '00013967d8af7b0001')
+
+To retrieve fingerprints for a given slot:
+
+>>> fps = [req['requests'] for req in frontier.read('test', 'example.com')]
 
 .. _Scrapinghub API: http://doc.scrapinghub.com/api.html
 .. _Collections: http://doc.scrapinghub.com/api/collections.html
+.. _Frontier: http://doc.scrapinghub.com/api/frontier.html
 
diff --git hubstorage/batchuploader.py hubstorage/batchuploader.py
index 1337e8d..b991541 100644
--- hubstorage/batchuploader.py
+++ hubstorage/batchuploader.py
@@ -3,12 +3,14 @@ import socket
 import random
 import logging
 import warnings
+import six
+from six.moves import range
+from six.moves.queue import Queue
+from io import BytesIO
 from gzip import GzipFile
 from itertools import count
 import requests
-from requests.compat import StringIO
 from collections import deque
-from Queue import Queue
 from threading import Thread, Event
 from .utils import xauth, iterqueue
 from .serialization import jsonencode
@@ -91,7 +93,7 @@ class BatchUploader(object):
                 continue
 
             # Delay once all writers are processed
-            if (ctr.next() % len(self._writers) == 0) and not self.closed:
+            if (next(ctr) % len(self._writers) == 0) and not self.closed:
                 self._interruptable_sleep()
 
             # Get next writer to process
@@ -125,12 +127,12 @@ class BatchUploader(object):
                 'content-encoding': w.content_encoding,
             })
             w.offset += qiter.count
-            for _ in xrange(qiter.count):
+            for _ in range(qiter.count):
                 q.task_done()
             if w.callback is not None:
                 try:
                     w.callback(response)
-                except Exception, e:
+                except Exception:
                     logger.exception("Callback for %s failed", w.url)
 
     def _content_encode(self, qiter, w):
@@ -148,12 +150,12 @@ class BatchUploader(object):
         Use polinomial backoff with 10 minutes maximum interval that accounts
         for ~30 hours of total retry time.
 
-        >>> sum(min(x**2, 600) for x in xrange(200)) / 3600
+        >>> sum(min(x**2, 600) for x in range(200)) / 3600
         30
         """
         url = batch['url']
         offset = batch['offset']
-        for retryn in xrange(self.worker_max_retries):
+        for retryn in range(self.worker_max_retries):
             emsg = ''
             try:
                 r = self._upload(batch)
@@ -229,7 +231,7 @@ class _BatchWriter(object):
         self.itemsq.put(data)
         if self.itemsq.full():
             self.uploader.interrupt()
-        return self._nextid.next()
+        return next(self._nextid)
 
     def flush(self):
         self.flushme = True
@@ -249,18 +251,22 @@ class _BatchWriter(object):
         return self.url
 
 
-def _encode_identity(iter):
-    data = StringIO()
-    for item in iter:
+def _encode_identity(iterable):
+    data = BytesIO()
+    for item in iterable:
+        if isinstance(item, six.text_type):
+            item = item.encode('utf8')
         data.write(item)
-        data.write('\n')
+        data.write(b'\n')
     return data.getvalue()
 
 
-def _encode_gzip(iter):
-    data = StringIO()
+def _encode_gzip(iterable):
+    data = BytesIO()
     with GzipFile(fileobj=data, mode='w') as gzo:
-        for item in iter:
+        for item in iterable:
+            if isinstance(item, six.text_type):
+                item = item.encode('utf8')
             gzo.write(item)
-            gzo.write('\n')
+            gzo.write(b'\n')
     return data.getvalue()
diff --git hubstorage/client.py hubstorage/client.py
index 83582c5..9899c0b 100644
--- hubstorage/client.py
+++ hubstorage/client.py
@@ -88,10 +88,12 @@ class HubstorageClient(object):
         def invoke_request():
             r = self.session.request(**kwargs)
 
-            if not r.ok:
+            try:
+                r.raise_for_status()
+                return r
+            except HTTPError:
                 logger.debug('%s: %s', r, r.content)
-            r.raise_for_status()
-            return r
+                raise
 
         if is_idempotent:
             return self.retrier.call(invoke_request)
diff --git hubstorage/collectionsrt.py hubstorage/collectionsrt.py
index 8f2b026..8815bde 100644
--- hubstorage/collectionsrt.py
+++ hubstorage/collectionsrt.py
@@ -11,7 +11,7 @@ class Collections(DownloadableResource):
     def get(self, _type, _name, _key=None, **params):
         try:
             r = self.apiget((_type, _name, _key), params=params)
-            return r if _key is None else r.next()
+            return r if _key is None else next(r)
         except HTTPError as exc:
             if exc.response.status_code == 404:
                 raise KeyError(_key)
@@ -78,8 +78,7 @@ class Collections(DownloadableResource):
         getparams = dict(params)
         try:
             while True:
-                r = self.apirequest(path, method=method,
-                    params=getparams).next()
+                r = next(self.apirequest(path, method=method, params=getparams))
                 total += r[total_param]
                 next = r.get('nextstart')
                 if next is None:
diff --git hubstorage/frontier.py hubstorage/frontier.py
index 68321e8..6f74451 100644
--- hubstorage/frontier.py
+++ hubstorage/frontier.py
@@ -1,4 +1,3 @@
-import json
 
 from .resourcetype import ResourceType
 from .utils import urlpathjoin
@@ -38,7 +37,7 @@ class Frontier(ResourceType):
         return writer
 
     def _writer_callback(self, response):
-        self.newcount += json.loads(response.content)["newcount"]
+        self.newcount += response.json()["newcount"]
 
     def close(self, block=True):
         for writer in self._writers.values():
diff --git hubstorage/jobq.py hubstorage/jobq.py
index cc179cf..bae12a2 100644
--- hubstorage/jobq.py
+++ hubstorage/jobq.py
@@ -31,6 +31,13 @@ class JobQ(ResourceType):
                 raise DuplicateJobError()
             raise
 
+    def jobsummary(self, jobkeys, jobmeta):
+        """Fetch selected job metadata fields for selected jobs."""
+        if not isinstance(jobkeys, (list, tuple)):
+            raise TypeError("jobkeys must be a list or a tuple")
+        return self.apiget(('jobsummary',),
+                           params={'key': jobkeys, 'jobmeta': jobmeta})
+
     def summary(self, _queuename=None, spiderid=None, count=None, start=None, jobmeta=None):
         params = {}
         if count is not None:
diff --git hubstorage/project.py hubstorage/project.py
index f9cacab..4c4a5ff 100644
--- hubstorage/project.py
+++ hubstorage/project.py
@@ -98,7 +98,7 @@ class Ids(ResourceType):
 
     def spider(self, spidername, **params):
         r = self.apiget(('spider', spidername), params=params)
-        return r.next()
+        return next(r)
 
 
 class Settings(MappingResourceType):
diff --git hubstorage/resourcetype.py hubstorage/resourcetype.py
index 7023fb2..5baddc3 100644
--- hubstorage/resourcetype.py
+++ hubstorage/resourcetype.py
@@ -1,3 +1,5 @@
+import six
+from six.moves import range
 import logging, time, json, socket
 from collections import MutableMapping
 import requests.exceptions as rexc
@@ -26,7 +28,10 @@ class ResourceType(object):
 
         r = self.client.request(**kwargs)
 
-        return r.iter_lines()
+        lines = r.iter_lines()
+        if six.PY3:
+            return (l.decode(r.encoding or 'utf8') for l in lines)
+        return lines
 
     def apirequest(self, _path=None, **kwargs):
         return jldecode(self._iter_lines(_path, **kwargs))
@@ -77,7 +82,7 @@ class DownloadableResource(ResourceType):
         lastexc = None
         line = None
         offset = 0
-        for attempt in xrange(self.MAX_RETRIES):
+        for attempt in range(self.MAX_RETRIES):
             self._add_resume_param(line, offset, apiparams)
             try:
                 for line in self._iter_lines(_path=_path, params=apiparams,
@@ -153,7 +158,7 @@ class ItemsResourceType(ResourceType):
             return o
 
     def stats(self):
-        return self.apiget('stats').next()
+        return next(self.apiget('stats'))
 
 
 class MappingResourceType(ResourceType, MutableMapping):
@@ -177,7 +182,7 @@ class MappingResourceType(ResourceType, MutableMapping):
         if self._cached is None:
             r = self.apiget()
             try:
-                self._cached = r.next()
+                self._cached = next(r)
             except StopIteration:
                 self._cached = {}
 
@@ -194,8 +199,8 @@ class MappingResourceType(ResourceType, MutableMapping):
             if not self.ignore_fields:
                 self.apipost(jl=self._data, is_idempotent=True)
             else:
-                self.apipost(jl=dict((k, v) for k, v in self._data.iteritems()
-                                     if k not in self.ignore_fields),
+                self.apipost(jl={k: v for k, v in six.iteritems(self._data)
+                                 if k not in self.ignore_fields},
                              is_idempotent=True)
 
     def __getitem__(self, key):
diff --git hubstorage/serialization.py hubstorage/serialization.py
index db359c4..3e6e6bc 100644
--- hubstorage/serialization.py
+++ hubstorage/serialization.py
@@ -1,3 +1,4 @@
+import six
 from json import dumps, loads
 from datetime import datetime
 
@@ -6,7 +7,7 @@ ADAYINSECONDS = 24 * 3600
 
 
 def jlencode(iterable):
-    if isinstance(iterable, (dict, str, unicode)):
+    if isinstance(iterable, (dict, six.string_types)):
         iterable = [iterable]
     return u'\n'.join(jsonencode(o) for o in iterable)
 
@@ -26,8 +27,6 @@ def jsondefault(o):
         u = delta.microseconds
         s = delta.seconds
         d = delta.days
-        millis = (u + (s + d * ADAYINSECONDS) * 1e6) / 1000
-        return int(millis)
+        return (u + (s + d * ADAYINSECONDS) * 1e6) // 1000
     else:
-        return str(o)
-
+        return six.text_type(o)
diff --git hubstorage/utils.py hubstorage/utils.py
index b34dadb..5a08a83 100644
--- hubstorage/utils.py
+++ hubstorage/utils.py
@@ -1,5 +1,6 @@
+import six
 import time
-from Queue import Empty
+from six.moves.queue import Empty
 
 
 def urlpathjoin(*parts):
@@ -35,12 +36,10 @@ def urlpathjoin(*parts):
             continue
         elif isinstance(p, tuple):
             p = urlpathjoin(*p)
-        elif isinstance(p, unicode):
-            p = p.encode('utf8')
-        elif not isinstance(p, str):
-            p = str(p)
+        elif not isinstance(p, six.text_type):
+            p = six.text_type(p)
 
-        url = p if url is None else '{0}/{1}'.format(url.rstrip('/'), p)
+        url = p if url is None else u'{0}/{1}'.format(url.rstrip(u'/'), p)
 
     return url
 
@@ -81,9 +80,9 @@ class iterqueue(object):
 
     it exposes an attribute "count" with the number of messages read
 
-    >>> from Queue import Queue
+    >>> from six.moves.queue import Queue
     >>> q = Queue()
-    >>> for x in xrange(10):
+    >>> for x in range(10):
     ...     q.put(x)
     >>> qiter = iterqueue(q)
     >>> list(qiter)
@@ -91,7 +90,7 @@ class iterqueue(object):
     >>> qiter.count
     10
 
-    >>> for x in xrange(10):
+    >>> for x in range(10):
     ...     q.put(x)
     >>> qiter = iterqueue(q, maxcount=4)
     >>> list(qiter)
diff --git setup.py setup.py
index e67af07..4550a48 100644
--- setup.py
+++ setup.py
@@ -16,7 +16,7 @@ setup(name='hubstorage',
       platforms=['Any'],
       packages=find_packages(),
       package_data={'hubstorage': ['VERSION']},
-      install_requires=['requests', 'retrying>=1.3.3'],
+      install_requires=['requests', 'retrying>=1.3.3', 'six>=1.10.0'],
       classifiers=['Development Status :: 4 - Beta',
                    'License :: OSI Approved :: BSD License',
                    'Operating System :: OS Independent',
diff --git tests/hstestcase.py tests/hstestcase.py
index e096d04..a673c7c 100644
--- tests/hstestcase.py
+++ tests/hstestcase.py
@@ -1,7 +1,9 @@
 import os
 import unittest
 import random
+import requests
 from hubstorage import HubstorageClient
+from hubstorage.utils import urlpathjoin
 
 
 class HSTestCase(unittest.TestCase):
@@ -12,7 +14,7 @@ class HSTestCase(unittest.TestCase):
     auth = os.getenv('HS_AUTH', 'f' * 32)
     frontier = 'test'
     slot = 'site.com'
-    testbotgroup = 'python-hubstorage-test'
+    testbotgroups = ['python-hubstorage-test', 'g1']
 
     @classmethod
     def setUpClass(cls):
@@ -37,7 +39,7 @@ class HSTestCase(unittest.TestCase):
     @classmethod
     def _remove_all_jobs(cls):
         project = cls.project
-        for k in project.settings.keys():
+        for k in list(project.settings.keys()):
             if k != 'botgroups':
                 del project.settings[k]
         project.settings.save()
@@ -53,6 +55,7 @@ class HSTestCase(unittest.TestCase):
 
     @classmethod
     def _remove_job(cls, jobkey):
+        cls.project.jobq.finish(jobkey)
         cls.project.jobq.delete(jobkey)
         cls._delete_job(jobkey)
 
@@ -63,13 +66,22 @@ class HSTestCase(unittest.TestCase):
 
     @classmethod
     def _set_testbotgroup(cls):
-        cls.project.settings.apipost(jl={'botgroups': [cls.testbotgroup]})
+        cls.project.settings.apipost(jl={'botgroups': [cls.testbotgroups[0]]})
+        # Additional step to populate JobQ's botgroups table
+        for botgroup in cls.testbotgroups:
+            url = urlpathjoin(cls.endpoint, 'botgroups',
+                              botgroup, 'max_running')
+            requests.post(url, auth=cls.project.auth, data='null')
         cls.project.settings.expire()
 
     @classmethod
     def _unset_testbotgroup(cls):
         cls.project.settings.apidelete('botgroups')
         cls.project.settings.expire()
+        # Additional step to delete botgroups in JobQ
+        for botgroup in cls.testbotgroups:
+            url = urlpathjoin(cls.endpoint, 'botgroups', botgroup)
+            requests.delete(url, auth=cls.project.auth)
 
     def start_job(self, **startparams):
         jobdata = self.project.jobq.start(**startparams)
diff --git tests/test_activity.py tests/test_activity.py
index 0189b3d..302cf0a 100644
--- tests/test_activity.py
+++ tests/test_activity.py
@@ -1,14 +1,15 @@
 """
 Test Activty
 """
-from hstestcase import HSTestCase
+from .hstestcase import HSTestCase
+from six.moves import range
 
 
 class ActivityTest(HSTestCase):
 
     def test_post_and_reverse_get(self):
         # make some sample data
-        orig_data = [{u'foo': 42, u'counter': i} for i in xrange(20)]
+        orig_data = [{u'foo': 42, u'counter': i} for i in range(20)]
         data1 = orig_data[:10]
         data2 = orig_data[10:]
 
@@ -22,12 +23,12 @@ class ActivityTest(HSTestCase):
         self.assertEqual(orig_data[::-1], result)
 
     def test_filters(self):
-        self.project.activity.post({'c': i} for i in xrange(10))
+        self.project.activity.post({'c': i} for i in range(10))
         r = list(self.project.activity.list(filter='["c", ">", [5]]', count=2))
         self.assertEqual(r, [{'c': 9}, {'c': 8}])
 
     def test_timestamp(self):
         self.project.activity.add({'foo': 'bar'}, baz='qux')
-        entry = self.project.activity.list(count=1, meta='_ts').next()
+        entry = next(self.project.activity.list(count=1, meta='_ts'))
         self.assertTrue(entry.pop('_ts', None))
         self.assertEqual(entry, {'foo': 'bar', 'baz': 'qux'})
diff --git tests/test_batchuploader.py tests/test_batchuploader.py
index 07d7dd3..77c5172 100644
--- tests/test_batchuploader.py
+++ tests/test_batchuploader.py
@@ -2,8 +2,9 @@
 Test Project
 """
 import time
+from six.moves import range
 from collections import defaultdict
-from hstestcase import HSTestCase
+from .hstestcase import HSTestCase
 from hubstorage import ValueTooLarge
 
 
@@ -18,7 +19,7 @@ class BatchUploaderTest(HSTestCase):
 
     def test_writer_batchsize(self):
         job, w = self._job_and_writer(size=10)
-        for x in xrange(111):
+        for x in range(111):
             w.write({'x': x})
         w.close()
         # this works only for small batches (previous size=10 and small data)
@@ -47,19 +48,19 @@ class BatchUploaderTest(HSTestCase):
             ValueTooLarge,
             'Value exceeds max encoded size of 1048576 bytes:'
             ' \'{"b+\\.\\.\\.\'',
-            w.write, {'b'*(m/2): 'x'*(m/2)})
+            w.write, {'b'*(m//2): 'x'*(m//2)})
 
     def test_writer_contentencoding(self):
         for ce in ('identity', 'gzip'):
             job, w = self._job_and_writer(content_encoding=ce)
-            for x in xrange(111):
+            for x in range(111):
                 w.write({'x': x})
             w.close()
             self.assertEqual(job.items.stats()['totals']['input_values'], 111)
 
     def test_writer_interval(self):
         job, w = self._job_and_writer(size=1000, interval=1)
-        for x in xrange(111):
+        for x in range(111):
             w.write({'x': x})
             if x == 50:
                 time.sleep(2)
diff --git tests/test_client.py tests/test_client.py
index 52527c7..00e3597 100644
--- tests/test_client.py
+++ tests/test_client.py
@@ -1,7 +1,7 @@
 """
 Test Client
 """
-from hstestcase import HSTestCase
+from .hstestcase import HSTestCase
 from hubstorage.utils import millitime, apipoll
 
 class ClientTest(HSTestCase):
@@ -15,6 +15,7 @@ class ClientTest(HSTestCase):
         m = job.metadata
         self.assertEqual(m.get('state'), u'running', c.auth)
         self.assertEqual(m.get('foo'), u'baz')
+        self.project.jobq.finish(job)
         self.project.jobq.delete(job)
 
         # job auth token is valid only while job is running
diff --git tests/test_collections.py tests/test_collections.py
index 1d2ab89..512d15a 100644
--- tests/test_collections.py
+++ tests/test_collections.py
@@ -2,9 +2,10 @@
 Test Collections
 """
 import random
+from six.moves import range
 from contextlib import closing
-from hstestcase import HSTestCase
-from testutil import failing_downloader
+from .hstestcase import HSTestCase
+from .testutil import failing_downloader
 
 
 def _mkitem():
@@ -45,7 +46,7 @@ class CollectionsTest(HSTestCase):
         test_item = _mkitem()
         last_key = None
         with closing(col.create_writer()) as writer:
-            for i in xrange(20):
+            for i in range(20):
                 test_item['_key'] = last_key = "post_scan_test%d" % i
                 test_item['counter'] = i
                 writer.write(test_item)
@@ -66,7 +67,7 @@ class CollectionsTest(HSTestCase):
         self.assertEqual(len(result), 10)
 
         # bulk delete
-        col.delete('post_scan_test%d' % i for i in xrange(20))
+        col.delete('post_scan_test%d' % i for i in range(20))
 
         # test items removed (check first and last)
         self.assertRaises(KeyError, col.get, 'post_scan_test0')
@@ -84,7 +85,7 @@ class CollectionsTest(HSTestCase):
         col = self.project.collections.new_store(self.test_collection_name)
         items = []
         with closing(col.create_writer()) as writer:
-            for i in xrange(20):
+            for i in range(20):
                 test_item = _mkitem()
                 test_item['_key'] = "test_data_download%d" % i
                 test_item['counter'] = i
diff --git tests/test_frontier.py tests/test_frontier.py
index 50b3a7a..e781377 100644
--- tests/test_frontier.py
+++ tests/test_frontier.py
@@ -1,7 +1,7 @@
 """
 Test Frontier
 """
-from hstestcase import HSTestCase
+from .hstestcase import HSTestCase
 
 
 class FrontierTest(HSTestCase):
diff --git tests/test_jobq.py tests/test_jobq.py
index 42a1f26..4f1e262 100644
--- tests/test_jobq.py
+++ tests/test_jobq.py
@@ -2,9 +2,11 @@
 Test JobQ
 """
 import os, unittest
-from hstestcase import HSTestCase
+import six
+from six.moves import range
 from hubstorage.jobq import DuplicateJobError
 from hubstorage.utils import apipoll
+from .hstestcase import HSTestCase
 
 
 EXCLUSIVE = os.environ.get('EXCLUSIVE_STORAGE')
@@ -97,10 +99,10 @@ class JobqTest(HSTestCase):
             'nil': None,
         }
         qj = jobq.push(self.spidername, **pushextras)
-        startextras = dict(('s_' + k, v) for k, v in pushextras.iteritems())
+        startextras = dict(('s_' + k, v) for k, v in six.iteritems(pushextras))
         nj = jobq.start(**startextras)
         self.assertEqual(qj['key'], nj['key'])
-        for k, v in dict(pushextras, **startextras).iteritems():
+        for k, v in six.iteritems(dict(pushextras, **startextras)):
             if type(v) is float:
                 self.assertAlmostEqual(nj.get(k), v)
             else:
@@ -142,10 +144,10 @@ class JobqTest(HSTestCase):
 
     def test_summary_countstart(self):
         # push more than 5 jobs into same queue
-        N = 20
+        N = 6
         jobq = self.project.jobq
         for state in ('pending', 'running', 'finished'):
-            for idx in xrange(N):
+            for idx in range(N):
                 jobq.push(self.spidername, state=state, idx=idx)
 
             s1 = jobq.summary(state)
@@ -178,6 +180,8 @@ class JobqTest(HSTestCase):
         self._assert_queue('running', [j1])
         self._assert_queue('finished', [j4, j2])
         # delete all jobs and check for empty summaries
+        jobq.finish(j1)
+        jobq.finish(j3)
         jobq.delete(j1)
         jobq.delete(j2)
         jobq.delete(j3)
@@ -293,11 +297,18 @@ class JobqTest(HSTestCase):
     def test_spider_updates(self):
         jobq = self.project.jobq
         spiderkey = '%s/%s' % (self.projectid, self.spiderid)
+
+        def finish_and_delete_jobs():
+            for job in jobq.finish(spiderkey):
+                yield job
+            jobq.delete(spiderkey)
+
         q1 = jobq.push(self.spidername)
         q2 = jobq.push(self.spidername, state='running')
         q3 = jobq.push(self.spidername, state='finished')
         q4 = jobq.push(self.spidername, state='deleted')
-        r = dict((x['key'], x['prevstate']) for x in jobq.delete(spiderkey))
+
+        r = dict((x['key'], x['prevstate']) for x in finish_and_delete_jobs())
         self.assertEqual(r.get(q1['key']), 'pending', r)
         self.assertEqual(r.get(q2['key']), 'running', r)
         self.assertEqual(r.get(q3['key']), 'finished', r)
@@ -327,6 +338,17 @@ class JobqTest(HSTestCase):
         self.assertEqual(job.metadata['state'], 'running')
         self.assertEqual(job.metadata['foo'], 'bar')
 
+    def test_jobsummary(self):
+        jobs = [self.project.push_job(self.spidername, foo=i)
+                for i in range(5)]
+        jobmetas = list(self.project.jobq.jobsummary(
+            jobkeys=[j.key for j in jobs], jobmeta=['key', 'foo']))
+        jobmeta_dict = {jm['key']: jm['foo'] for jm in jobmetas}
+        assert jobmeta_dict == {
+            jobs[i].key: i
+            for i in range(5)
+        }
+
 
 def _keys(lst):
     return [x['key'] for x in lst]
diff --git tests/test_jobsmeta.py tests/test_jobsmeta.py
index b2254c4..c2bfbe6 100644
--- tests/test_jobsmeta.py
+++ tests/test_jobsmeta.py
@@ -3,7 +3,7 @@ Test job metadata
 
 System tests for operations on stored job metadata
 """
-from hstestcase import HSTestCase
+from .hstestcase import HSTestCase
 
 
 class JobsMetadataTest(HSTestCase):
diff --git tests/test_project.py tests/test_project.py
index ff39e39..8d61c25 100644
--- tests/test_project.py
+++ tests/test_project.py
@@ -2,12 +2,14 @@
 Test Project
 """
 import json
+import six
+from six.moves import range
 from random import randint, random
 from requests.exceptions import HTTPError
 from hubstorage import HubstorageClient
 from hubstorage.utils import millitime
-from hstestcase import HSTestCase
-from testutil import failing_downloader
+from .hstestcase import HSTestCase
+from .testutil import failing_downloader
 
 
 class ProjectTest(HSTestCase):
@@ -16,8 +18,8 @@ class ProjectTest(HSTestCase):
         p1 = self.hsclient.get_project(int(self.projectid))
         p2 = self.hsclient.get_project(str(self.projectid))
         self.assertEqual(p1.projectid, p2.projectid)
-        self.assertEqual(type(p1.projectid), str)
-        self.assertEqual(type(p2.projectid), str)
+        self.assertEqual(type(p1.projectid), six.text_type)
+        self.assertEqual(type(p2.projectid), six.text_type)
         self.assertRaises(AssertionError, self.hsclient.get_project, '111/3')
 
     def test_get_job_from_key(self):
@@ -69,6 +71,7 @@ class ProjectTest(HSTestCase):
                                    foo=u'bar')
         self.assertEqual(job.metadata.get('state'), u'running')
         self.assertEqual(job.metadata.get('foo'), u'bar')
+        self.project.jobq.finish(job)
         self.project.jobq.delete(job)
         job.metadata.expire()
         self.assertEqual(job.metadata.get('state'), u'deleted')
@@ -172,21 +175,21 @@ class ProjectTest(HSTestCase):
 
         job.requests.close()
         rr = job.requests.list()
-        self.assertEqual(rr.next(),
+        self.assertEqual(next(rr),
                          {u'status': 200, u'rs': 1337,
                           u'url': u'http://test.com/', u'time': ts,
                           u'duration': 5, u'method': u'GET'})
-        self.assertEqual(rr.next(),
+        self.assertEqual(next(rr),
                          {u'status': 400, u'parent': 0, u'rs': 0,
                           u'url': u'http://test.com/2', u'time': ts + 1,
                           u'duration': 1, u'method': u'POST'})
-        self.assertEqual(rr.next(),
+        self.assertEqual(next(rr),
                          {u'status': 400, u'fp': u'1234', u'parent': 0,
                           u'rs': 0, u'url': u'http://test.com/3',
                           u'time': ts + 2, u'duration': 1,
                           u'method': u'PUT'})
 
-        self.assertRaises(StopIteration, rr.next)
+        self.assertRaises(StopIteration, next, rr)
 
     def test_samples(self):
         # no samples stored
@@ -207,9 +210,9 @@ class ProjectTest(HSTestCase):
         samples = []
         ts = millitime()
         count = int(j2.samples.batch_size * (random() + randint(1, 5)))
-        for _ in xrange(count):
+        for _ in range(count):
             ts += randint(1, 2**16)
-            row = [ts] + list(randint(0, 2**16) for _ in xrange(randint(0, 100)))
+            row = [ts] + list(randint(0, 2**16) for _ in range(randint(0, 100)))
             samples.append(row)
             j2.samples.write(row)
         j2.samples.flush()
@@ -227,7 +230,7 @@ class ProjectTest(HSTestCase):
 
     def test_bulkdata(self):
         j = self.project.push_job(self.spidername, state='running')
-        for i in xrange(20):
+        for i in range(20):
             j.logs.info("log line %d" % i)
             j.items.write(dict(field1="item%d" % i))
             j.requests.add("http://test.com/%d" % i,
@@ -240,3 +243,12 @@ class ProjectTest(HSTestCase):
             with failing_downloader(resource):
                 downloaded = list(resource.iter_values())
                 self.assertEqual(len(downloaded), 20)
+
+    def test_output_string(self):
+        project = self.hsclient.get_project(self.projectid)
+        project.push_job(self.spidername)
+        job = self.start_job()
+        job.items.write({'foo': 'bar'})
+        job.close_writers()
+        items = self.hsclient.get_job(job.key).items.iter_json()
+        self.assertEqual(type(next(items)), str)
diff --git tests/test_retry.py tests/test_retry.py
index 424dcf4..91b7de4 100644
--- tests/test_retry.py
+++ tests/test_retry.py
@@ -1,9 +1,9 @@
 """
 Test Retry Policy
 """
-from httplib import BadStatusLine
+from six.moves.http_client import BadStatusLine
 from requests import HTTPError, ConnectionError
-from hstestcase import HSTestCase
+from .hstestcase import HSTestCase
 from hubstorage import HubstorageClient
 import responses
 import json
@@ -42,7 +42,7 @@ class RetryTest(HSTestCase):
     @responses.activate
     def test_retrier_does_not_catch_unwanted_exception(self):
         # Prepare
-        client = HubstorageClient(auth=self.auth, endpoint=self.endpoint, max_retries=2)
+        client = HubstorageClient(auth=self.auth, endpoint=self.endpoint, max_retries=2, max_retry_time=1)
         job_metadata = {'project': self.projectid, 'spider': self.spidername, 'state': 'pending'}
         callback, attempts_count = self.make_request_callback(3, job_metadata, http_error_status=403)
 
@@ -65,7 +65,7 @@ class RetryTest(HSTestCase):
     @responses.activate
     def test_retrier_catches_badstatusline_and_429(self):
         # Prepare
-        client = HubstorageClient(auth=self.auth, endpoint=self.endpoint, max_retries=3)
+        client = HubstorageClient(auth=self.auth, endpoint=self.endpoint, max_retries=3, max_retry_time=1)
         job_metadata = {'project': self.projectid, 'spider': self.spidername, 'state': 'pending'}
 
         attempts_count = [0]  # use a list for nonlocal mutability used in request_callback
@@ -76,7 +76,7 @@ class RetryTest(HSTestCase):
             if attempts_count[0] <= 2:
                 raise ConnectionError("Connection aborted.", BadStatusLine("''"))
             if attempts_count[0] == 3:
-                return (429, {}, {})
+                return (429, {}, u'')
             else:
                 resp_body = dict(job_metadata)
                 return (200, {}, json.dumps(resp_body))
@@ -93,7 +93,7 @@ class RetryTest(HSTestCase):
     @responses.activate
     def test_api_delete_can_be_set_to_non_idempotent(self):
         # Prepare
-        client = HubstorageClient(auth=self.auth, endpoint=self.endpoint, max_retries=3)
+        client = HubstorageClient(auth=self.auth, endpoint=self.endpoint, max_retries=3, max_retry_time=1)
         job_metadata = {'project': self.projectid, 'spider': self.spidername, 'state': 'pending'}
         callback_delete, attempts_count_delete = self.make_request_callback(2, job_metadata)
 
@@ -115,7 +115,7 @@ class RetryTest(HSTestCase):
     @responses.activate
     def test_collection_store_and_delete_are_retried(self):
         # Prepare
-        client = HubstorageClient(auth=self.auth, endpoint=self.endpoint, max_retries=3)
+        client = HubstorageClient(auth=self.auth, endpoint=self.endpoint, max_retries=3, max_retry_time=1)
 
         callback_post, attempts_count_post = self.make_request_callback(2, [])
         callback_delete, attempts_count_delete = self.make_request_callback(2, [])
@@ -136,7 +136,7 @@ class RetryTest(HSTestCase):
     @responses.activate
     def test_delete_requests_are_retried(self):
         # Prepare
-        client = HubstorageClient(auth=self.auth, endpoint=self.endpoint, max_retries=3)
+        client = HubstorageClient(auth=self.auth, endpoint=self.endpoint, max_retries=3, max_retry_time=1)
         job_metadata = {'project': self.projectid, 'spider': self.spidername, 'state': 'pending'}
         callback_getpost, attempts_count_getpost = self.make_request_callback(0, job_metadata)
         callback_delete, attempts_count_delete = self.make_request_callback(2, job_metadata)
@@ -157,7 +157,7 @@ class RetryTest(HSTestCase):
     @responses.activate
     def test_metadata_save_does_retry(self):
         # Prepare
-        client = HubstorageClient(auth=self.auth, endpoint=self.endpoint, max_retries=3)
+        client = HubstorageClient(auth=self.auth, endpoint=self.endpoint, max_retries=3, max_retry_time=1)
         job_metadata = {'project': self.projectid, 'spider': self.spidername, 'state': 'pending'}
         callback_get, attempts_count_get = self.make_request_callback(0, job_metadata)
         callback_post, attempts_count_post = self.make_request_callback(2, job_metadata)
@@ -197,7 +197,7 @@ class RetryTest(HSTestCase):
     @responses.activate
     def test_get_job_does_retry(self):
         # Prepare
-        client = HubstorageClient(auth=self.auth, endpoint=self.endpoint, max_retries=3)
+        client = HubstorageClient(auth=self.auth, endpoint=self.endpoint, max_retries=3, max_retry_time=1)
         job_metadata = {'project': self.projectid, 'spider': self.spidername, 'state': 'pending'}
         callback, attempts_count = self.make_request_callback(2, job_metadata)
 
@@ -236,7 +236,7 @@ class RetryTest(HSTestCase):
     @responses.activate
     def test_get_job_does_fails_on_too_many_retries(self):
         # Prepare
-        client = HubstorageClient(auth=self.auth, endpoint=self.endpoint, max_retries=2)
+        client = HubstorageClient(auth=self.auth, endpoint=self.endpoint, max_retries=2, max_retry_time=1)
         job_metadata = {'project': self.projectid, 'spider': self.spidername, 'state': 'pending'}
         callback, attempts_count = self.make_request_callback(3, job_metadata)
 
diff --git tests/test_system.py tests/test_system.py
index 4b20d8e..c6b6e8e 100644
--- tests/test_system.py
+++ tests/test_system.py
@@ -1,6 +1,7 @@
 import random
+from six.moves import range
 from contextlib import closing
-from hstestcase import HSTestCase
+from .hstestcase import HSTestCase
 from hubstorage import HubstorageClient
 from hubstorage.utils import millitime
 
@@ -79,7 +80,7 @@ class SystemTest(HSTestCase):
         client = HubstorageClient(endpoint=self.endpoint)
         with closing(client) as scraperclient:
             job = scraperclient.get_job(jobkey, auth=jobauth)
-            for idx in xrange(self.MAGICN):
+            for idx in range(self.MAGICN):
                 iid = job.items.write({'uuid': idx})
                 job.logs.debug('log debug %s' % idx, idx=idx)
                 job.logs.info('log info %s' % idx, idx=idx)
